# API Specification

**Project:** Pydantic AI Agent API
**Base URL**: `http://localhost:8000`
**Protocol**: HTTP/1.1 with Server-Sent Events (SSE)
**Version**: 1.0.0

---

## Table of Contents

1. [Overview](#overview)
2. [Authentication](#authentication)
3. [Endpoints](#endpoints)
4. [Server-Sent Events (SSE) Format](#server-sent-events-sse-format)
5. [Error Handling](#error-handling)
6. [Code Examples](#code-examples)

---

## Overview

The Pydantic AI Agent API provides a streaming interface for conversational AI. The API is built with FastAPI and uses Server-Sent Events (SSE) for real-time token streaming.

### Key Features

- **Streaming Responses**: Token-by-token response delivery
- **CORS Enabled**: Configured for frontend at `http://localhost:3000`
- **Type-Safe**: Pydantic models for request/response validation
- **Health Checks**: Monitor API and agent status

---

## Authentication

**Current Version**: No authentication required

**Future**: JWT-based authentication will be added for multi-tenant support

---

## Endpoints

### 1. Chat Stream

Stream a conversation with the AI agent.

**Endpoint**: `POST /api/chat/stream`

**Request Headers**:
```
Content-Type: application/json
Accept: text/event-stream
```

**Request Body**:
```typescript
{
  "message": string  // User's message (required, 1-2000 chars)
}
```

**Example Request**:
```json
{
  "message": "What is the weather like today?"
}
```

**Response Type**: `text/event-stream` (SSE)

**Response Format**:
```
data: {"token": "Hello"}\n\n
data: {"token": " "}\n\n
data: {"token": "there"}\n\n
data: {"token": "!"}\n\n
data: {"done": true}\n\n
```

**Success Response (200 OK)**:
- Stream of SSE events
- Each event contains either:
  - `{"token": "text"}` - A piece of the response
  - `{"done": true}` - Stream completion marker

**Error Response (4xx/5xx)**:
```
data: {"error": "Error message"}\n\n
```

**Status Codes**:
- `200 OK` - Stream started successfully
- `400 Bad Request` - Invalid request body
- `422 Unprocessable Entity` - Validation error
- `500 Internal Server Error` - Agent error

---

### 2. Health Check

Check if the API and agent are healthy.

**Endpoint**: `GET /api/health`

**Request Headers**: None

**Response** (200 OK):
```json
{
  "status": "healthy",
  "agent": "ready"
}
```

**Status Codes**:
- `200 OK` - Service is healthy
- `503 Service Unavailable` - Service is not ready

---

## Server-Sent Events (SSE) Format

### What is SSE?

Server-Sent Events is a standard for pushing data from server to client over HTTP. It's perfect for one-way streaming (server â†’ client).

### Event Format

Each SSE event follows this format:

```
data: {JSON payload}\n\n
```

- **Prefix**: `data: `
- **Payload**: JSON-encoded data
- **Terminator**: Double newline (`\n\n`)

### Event Types

#### 1. Token Event

Sent for each token/piece of text generated by the AI.

```
data: {"token": "Hello"}\n\n
```

**Payload**:
```typescript
{
  "token": string  // Part of the AI response
}
```

#### 2. Completion Event

Sent when the AI has finished generating the response.

```
data: {"done": true}\n\n
```

**Payload**:
```typescript
{
  "done": true
}
```

#### 3. Error Event

Sent if an error occurs during processing.

```
data: {"error": "Something went wrong"}\n\n
```

**Payload**:
```typescript
{
  "error": string  // Error message
}
```

### Example SSE Stream

Complete stream for "Hello there!":

```
data: {"token": "Hello"}\n\n
data: {"token": " "}\n\n
data: {"token": "there"}\n\n
data: {"token": "!"}\n\n
data: {"done": true}\n\n
```

---

## Error Handling

### Validation Errors

**Status**: 422 Unprocessable Entity

**Example**:
```json
{
  "detail": [
    {
      "loc": ["body", "message"],
      "msg": "field required",
      "type": "value_error.missing"
    }
  ]
}
```

### Processing Errors

**Status**: 200 OK (streaming started)

**SSE Event**:
```
data: {"error": "Failed to process message"}\n\n
```

### Connection Errors

If the client loses connection, SSE will automatically attempt to reconnect.

---

## Code Examples

### cURL

**Chat Stream**:
```bash
curl -X POST http://localhost:8000/api/chat/stream \
  -H "Content-Type: application/json" \
  -H "Accept: text/event-stream" \
  -d '{"message": "Hello, AI!"}' \
  --no-buffer
```

**Health Check**:
```bash
curl http://localhost:8000/api/health
```

### JavaScript/TypeScript (Fetch API)

```typescript
async function streamChat(message: string) {
  const response = await fetch('http://localhost:8000/api/chat/stream', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({ message }),
  });

  const reader = response.body!
    .pipeThrough(new TextDecoderStream())
    .getReader();

  while (true) {
    const { value, done } = await reader.read();
    if (done) break;

    // Parse SSE format
    const lines = value.split('\n');
    for (const line of lines) {
      if (line.startsWith('data: ')) {
        const data = JSON.parse(line.slice(6));

        if (data.token) {
          console.log('Token:', data.token);
        } else if (data.done) {
          console.log('Stream complete');
        } else if (data.error) {
          console.error('Error:', data.error);
        }
      }
    }
  }
}

// Usage
streamChat('What is the capital of France?');
```

### JavaScript (EventSource - Alternative)

Note: EventSource doesn't support POST, so this is for GET endpoints only:

```javascript
const eventSource = new EventSource('http://localhost:8000/api/chat/stream');

eventSource.onmessage = (event) => {
  const data = JSON.parse(event.data);

  if (data.token) {
    console.log('Token:', data.token);
  } else if (data.done) {
    console.log('Complete');
    eventSource.close();
  }
};

eventSource.onerror = (error) => {
  console.error('Error:', error);
  eventSource.close();
};
```

### Python (httpx with streaming)

```python
import httpx
import json

async def stream_chat(message: str):
    async with httpx.AsyncClient() as client:
        async with client.stream(
            'POST',
            'http://localhost:8000/api/chat/stream',
            json={'message': message},
            timeout=None
        ) as response:
            async for line in response.aiter_lines():
                if line.startswith('data: '):
                    data = json.loads(line[6:])

                    if 'token' in data:
                        print(data['token'], end='', flush=True)
                    elif data.get('done'):
                        print('\n[Complete]')
                    elif 'error' in data:
                        print(f'\n[Error: {data["error"]}]')

# Usage
import asyncio
asyncio.run(stream_chat('Tell me a joke'))
```

### React Hook (TypeScript)

```typescript
import { useState, useCallback } from 'react';

interface UseStreamingChatResult {
  messages: Message[];
  streamingContent: string;
  isStreaming: boolean;
  sendMessage: (message: string) => Promise<void>;
  error: string | null;
}

export function useStreamingChat(): UseStreamingChatResult {
  const [messages, setMessages] = useState<Message[]>([]);
  const [streamingContent, setStreamingContent] = useState('');
  const [isStreaming, setIsStreaming] = useState(false);
  const [error, setError] = useState<string | null>(null);

  const sendMessage = useCallback(async (message: string) => {
    // Add user message
    setMessages(prev => [...prev, { role: 'user', content: message }]);
    setIsStreaming(true);
    setError(null);

    let fullResponse = '';

    try {
      const response = await fetch('http://localhost:8000/api/chat/stream', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ message }),
      });

      const reader = response.body!
        .pipeThrough(new TextDecoderStream())
        .getReader();

      while (true) {
        const { value, done } = await reader.read();
        if (done) break;

        const lines = value.split('\n');
        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = JSON.parse(line.slice(6));

            if (data.token) {
              fullResponse += data.token;
              setStreamingContent(fullResponse);
            } else if (data.done) {
              setMessages(prev => [...prev, {
                role: 'assistant',
                content: fullResponse
              }]);
              setStreamingContent('');
              setIsStreaming(false);
            } else if (data.error) {
              setError(data.error);
              setIsStreaming(false);
            }
          }
        }
      }
    } catch (err) {
      setError(err instanceof Error ? err.message : 'Unknown error');
      setIsStreaming(false);
    }
  }, []);

  return { messages, streamingContent, isStreaming, sendMessage, error };
}
```

---

## Request/Response Schemas

### ChatRequest

```typescript
interface ChatRequest {
  message: string;  // 1-2000 characters
}
```

**Validation Rules**:
- `message`: Required, non-empty string
- Max length: 2000 characters
- Min length: 1 character

### TokenEvent

```typescript
interface TokenEvent {
  token: string;  // Part of AI response
}
```

### DoneEvent

```typescript
interface DoneEvent {
  done: true;
}
```

### ErrorEvent

```typescript
interface ErrorEvent {
  error: string;  // Error description
}
```

### HealthResponse

```typescript
interface HealthResponse {
  status: 'healthy' | 'unhealthy';
  agent: 'ready' | 'initializing' | 'error';
}
```

---

## CORS Configuration

The API is configured to allow requests from:
- `http://localhost:3000` (Next.js frontend)

**Allowed Methods**: `GET`, `POST`, `OPTIONS`
**Allowed Headers**: `*`
**Credentials**: `true`

---

## Rate Limiting

**Current Version**: No rate limiting

**Future**: Rate limiting will be added:
- 100 requests per minute per IP
- 1000 requests per hour per user

---

## Performance Considerations

### Streaming Performance

- **Token Latency**: Typically 50-200ms between tokens
- **First Token**: Usually within 1-2 seconds
- **Connection Timeout**: 120 seconds (configurable)

### Optimization Tips

1. **Keep Connections Alive**: Reuse HTTP connections
2. **Handle Backpressure**: Ensure client consumes tokens quickly
3. **Error Recovery**: Implement retry logic for network errors
4. **Buffer Management**: Don't accumulate large buffers

---

## Testing

### Test Health Endpoint

```bash
curl http://localhost:8000/api/health
# Expected: {"status":"healthy","agent":"ready"}
```

### Test Streaming

```bash
curl -X POST http://localhost:8000/api/chat/stream \
  -H "Content-Type: application/json" \
  -d '{"message":"Count to 5"}' \
  --no-buffer
```

### Test Error Handling

```bash
# Invalid request (missing message)
curl -X POST http://localhost:8000/api/chat/stream \
  -H "Content-Type: application/json" \
  -d '{}'
```

---

## Changelog

### Version 1.0.0 (2025-11-04)
- Initial API release
- Chat streaming endpoint
- Health check endpoint
- SSE-based streaming
- CORS support

---

## Future Endpoints

### Planned for v1.1

- `POST /api/auth/login` - User authentication
- `GET /api/memory/{user_id}` - Retrieve user memories
- `GET /api/conversations` - List conversation history
- `DELETE /api/conversations/{id}` - Delete conversation

### Planned for v1.2

- `GET /api/graph/relationships` - GraphRAG visualization data
- `POST /api/tools/execute` - Execute custom tools
- `GET /api/models` - List available LLM models
- `PUT /api/settings` - Update agent configuration

---

**Last Updated**: 2025-11-04
**Maintained By**: Pydantic AI Agent Team
**API Version**: 1.0.0
